import os
from pathlib import Path
import torch
import pandas as pd
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, EncoderDecoderCache
from datasets import Dataset
import sacrebleu
import logging
from torch.utils.data import DataLoader

#######################
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda}")
print(f"Number of GPUs: {torch.cuda.device_count()}")
if torch.cuda.is_available():
    print(f"GPU Name: {torch.cuda.get_device_name(0)}")
#########################

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_and_tokenize(tokenizer, file_path, src_col, tgt_col, max_length=128, batch_size=4):
    # Load bitext CSV
    try:
        df = pd.read_csv(file_path)
        logger.info(f"Loaded CSV with columns: {list(df.columns)}")
    except Exception as e:
        logger.error(f"Failed to load CSV {file_path}: {str(e)}")
        return None

    # Validate column indices
    if src_col >= len(df.columns) or tgt_col >= len(df.columns):
        logger.error(f"Invalid column indices: src_col={src_col}, tgt_col={tgt_col}, but CSV has {len(df.columns)} columns")
        return None

    # Rename columns
    df = df.rename(columns={df.columns[src_col]: 'source', df.columns[tgt_col]: 'target'})

    # Convert to strings and handle missing values
    df['source'] = df['source'].astype(str).fillna('')
    df['target'] = df['target'].astype(str).fillna('')

    # Log sample data
    logger.info(f"Sample source data: {df['source'].head().tolist()}")
    logger.info(f"Sample target data: {df['target'].head().tolist()}")

    dataset = Dataset.from_pandas(df)

    def tokenize_batch(examples):
        source_texts = [str(text) for text in examples['source'] if str(text).strip()]
        target_texts = [str(text) for text in examples['target'] if str(text).strip()]
        
        if not source_texts or not target_texts:
            logger.warning("Empty batch after filtering, skipping")
            return {'input_ids': [], 'attention_mask': [], 'labels': []}

        logger.debug(f"Tokenizing source: {source_texts[:2]}")
        logger.debug(f"Tokenizing target: {target_texts[:2]}")

        inputs = tokenizer(source_texts, padding='max_length',
                          truncation=True, max_length=max_length, return_tensors='pt')
        labels = tokenizer(target_texts, padding='max_length',
                          truncation=True, max_length=max_length, return_tensors='pt')
        labels['input_ids'] = [
            [(tok if tok != tokenizer.pad_token_id else -100) for tok in label]
            for label in labels['input_ids']
        ]
        return {
            'input_ids': inputs['input_ids'],
            'attention_mask': inputs['attention_mask'],
            'labels': labels['input_ids']
        }

    tokenized_dataset = dataset.map(tokenize_batch, batched=True, remove_columns=dataset.column_names)
    tokenized_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])
    return DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=True)

def evaluate_and_save(model, tokenizer, eval_file, src_col, ref_col, output_path, device):
    from transformers import EncoderDecoderCache

    # Load evaluation CSV
    try:
        df = pd.read_csv(eval_file)
        logger.info(f"Loaded evaluation CSV with columns: {list(df.columns)}")
    except Exception as e:
        logger.error(f"Failed to load evaluation CSV {eval_file}: {str(e)}")
        return

    # Check column indices
    if src_col >= len(df.columns) or ref_col >= len(df.columns):
        logger.error(f"Invalid column indices: src_col={src_col}, ref_col={ref_col}, but CSV has {len(df.columns)} columns")
        return

    generated_texts = []
    bleu_scores = []

    model.eval()

    for i, row in df.iterrows():
        input_text = str(row.iloc[src_col]) if pd.notnull(row.iloc[src_col]) else ''

        if not input_text.strip():
            logger.warning(f"[{i}] Skipping empty input.")
            generated_texts.append('')
            bleu_scores.append(0.0)
            continue

        try:
            logger.debug(f"[{i}] Input text: {input_text}")

            inputs = tokenizer(
                input_text,
                return_tensors='pt',
                padding=True,
                truncation=True,
                max_length=128
            )
            inputs = {k: v.to(device) for k, v in inputs.items()}

            with torch.no_grad():
                output_ids = model.generate(
                    **inputs,
                    max_length=128,
                    num_beams=5,
                    early_stopping=True
                )

            logger.debug(f"[{i}] Generated token IDs: {output_ids}")
            generated = tokenizer.decode(output_ids[0], skip_special_tokens=True)

        except TypeError as e:
            # Patch deprecated past_key_values if necessary
            if "past_key_values" in str(e):
                logger.warning(f"[{i}] Switching to EncoderDecoderCache for past_key_values.")
                past_kv = kwargs.get("past_key_values")
                if isinstance(past_kv, tuple):
                    kwargs["past_key_values"] = EncoderDecoderCache.from_legacy_cache(past_kv)
                # Retry generation with patched cache
                with torch.no_grad():
                    output_ids = model.generate(
                        **inputs,
                        max_length=128,
                        num_beams=5,
                        early_stopping=True
                    )
                generated = tokenizer.decode(output_ids[0], skip_special_tokens=True)
            else:
                logger.error(f"[{i}] Generation error: {str(e)}")
                generated = ''

        except Exception as e:
            logger.error(f"[{i}] Error during generation: {str(e)}")
            generated = ''

        reference = str(row.iloc[ref_col]) if pd.notnull(row.iloc[ref_col]) else ''
        bleu = sacrebleu.sentence_bleu(generated, [reference]).score if reference.strip() else 0.0

        logger.info(f"[{i}]")
        logger.info(f"SRC: {input_text}")
        logger.info(f"GEN: {generated}")
        logger.info(f"REF: {reference}")
        logger.info(f"BLEU: {bleu:.2f}\n---")

        generated_texts.append(generated)
        bleu_scores.append(bleu)

    df['generated_zrl'] = generated_texts
    df['bleu_score'] = bleu_scores

    try:
        df.to_csv(output_path, index=False)
        logger.info(f"Evaluation results saved to: {output_path}")
    except Exception as e:
        logger.error(f"Failed to save results to CSV: {str(e)}")


def main():
    # Check versions
    try:
        import transformers
        import safetensors
        logger.info(f"Transformers version: {transformers.__version__}")
        logger.info(f"Torch version: {torch.__version__}")
        logger.info(f"Safetensors version: {safetensors.__version__}")
    except ImportError as e:
        logger.error(f"Library not found: {str(e)}. Install using: pip install transformers torch safetensors")
        return

    # Paths and filenames
    data_dir = Path(".")
    train_file = Path(r"C:\Users\dcurl\Desktop\Test\input_text.csv")
    eval_file = Path(r"C:\Users\dcurl\Desktop\Test\eval_text.csv")
    output_dir = Path(r"C:\Users\dcurl\Desktop\Test\output")
    output_dir.mkdir(exist_ok=True)

    # Model and tokenizer
    model_name = "facebook/nllb-200-3.3B"
    try:
        logger.info(f"Loading tokenizer for {model_name}")
        tokenizer = AutoTokenizer.from_pretrained(model_name, use_safetensors=True)
        if "[zrl]" not in tokenizer.get_vocab():
            tokenizer.add_tokens(["[zrl]"])
        logger.info(f"Loading model for {model_name}")
        # Explicitly avoid meta device by loading directly to CPU or CUDA
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name, use_safetensors=True, device_map=None)
        model.resize_token_embeddings(len(tokenizer))
        model.config.forced_bos_token_id = tokenizer.convert_tokens_to_ids("[zrl]")
    except Exception as e:
        logger.error(f"Failed to load model or tokenizer: {str(e)}")
        return

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"Using device: {device}")

    # Check if model is on meta device and move it properly
    if any(param.device.type == 'meta' for param in model.parameters()):
        logger.info("Model detected on meta device, moving to target device with to_empty")
        model = model.to_empty(device=device)
        # Initialize parameters after moving from meta device
        model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)
    else:
        logger.info("Model not on meta device, moving to target device")
        model.to(device)

    # Verify model device
    logger.info(f"Model parameters device: {next(model.parameters()).device}")

    # Prepare dataset
    train_dataloader = load_and_tokenize(
        tokenizer, train_file, src_col=4, tgt_col=5, batch_size=4)
    if train_dataloader is None:
        logger.error("Failed to create training dataloader")
        return

        # Training setup
    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)
    num_epochs = 10
    logging_steps = 50
    step = 0
    best_val_loss = float('inf')
    patience = 2
    patience_counter = 0

    # Split dataset for validation
    train_size = int(0.9 * len(train_dataloader.dataset))
    val_size = len(train_dataloader.dataset) - train_size
    train_subset, val_subset = torch.utils.data.random_split(train_dataloader.dataset, [train_size, val_size])
    train_loader = DataLoader(train_subset, batch_size=4, shuffle=True)
    val_loader = DataLoader(val_subset, batch_size=4)

    total_steps = len(train_loader) * num_epochs
    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.01, total_iters=total_steps)

    logger.info("Starting fine-tuning with validation...")

    model.train()
    for epoch in range(num_epochs):
        total_loss = 0.0
        for batch in train_loader:
            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}
            labels = batch['labels'].to(device)
            outputs = model(**inputs, labels=labels)
            loss = outputs.loss
            print(torch.cuda.memory_summary())
            loss.backward()
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()

            step += 1
            total_loss += loss.item()

            if step % logging_steps == 0:
                avg_loss = total_loss / logging_steps
                logger.info(f"Epoch {epoch+1}/{num_epochs}, Step {step}/{total_steps}, Loss: {avg_loss:.4f}")
                total_loss = 0.0

        # --- Validation phase ---
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for batch in val_loader:
                inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}
                labels = batch['labels'].to(device)
                outputs = model(**inputs, labels=labels)
                val_loss += outputs.loss.item()

        val_loss /= len(val_loader)
        logger.info(f"Validation Loss after Epoch {epoch+1}: {val_loss:.4f}")
        model.train()

        # Early stopping check
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            logger.info("Validation loss improved — keeping model.")
        else:
            patience_counter += 1
            logger.info(f"No improvement in validation loss (patience {patience_counter}/{patience})")
            if patience_counter >= patience:
                logger.info("Early stopping triggered.")
                break

    # Save model
    try:
        model.save_pretrained(output_dir / "fine_tuned_model")
        tokenizer.save_pretrained(output_dir / "fine_tuned_model")
        logger.info(f"Model saved to {output_dir / 'fine_tuned_model'}")
    except Exception as e:
        logger.error(f"Failed to save model: {str(e)}")

    # Evaluate and save
    output_eval = output_dir / "eval_results.csv"
    evaluate_and_save(
        model, tokenizer, eval_file,
        src_col=4, ref_col=5,
        output_path=output_eval,
        device=device
    )

if __name__ == "__main__":
    main()
