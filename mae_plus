import os
import logging
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, MarianMTModel, MarianTokenizer
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from tqdm import tqdm

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Configuration with 5 bitext sets
config = {
    'bitext_sets': [
        {'name': 'set1', 'input_csv': '/path/to/bitext_set1/input.csv', 'english_csv': '/path/to/bitext_set1/english.csv', 'output_csv': '/path/to/output_set1.csv charting'},
        {'name': 'set2', 'input_csv': '/path/to/bitext_set2/input.csv', 'english_csv': '/path/to/bitext_set2/english.csv', 'output_csv': '/path/to/output_set2.csv'},
        {'name': 'set3', 'input_csv': '/path/to/bitext_set3/input.csv', 'english_csv': '/path/to/bitext_set3/english.csv', 'output_csv': '/path/to/output_set3.csv'},
        {'name': 'set4', 'input_csv': '/path/to/bitext_set4/input.csv', 'english_csv': '/path/to/bitext_set4/english.csv', 'output_csv': '/path/to/output_set4.csv'},
        {'name': 'set5', 'input_csv': '/path/to/bitext_set5/input.csv', 'english_csv': '/path/to/bitext_set5/english.csv', 'output_csv': '/path/to/output_set5.csv'},
    ],
    'max_length': 50,
    'embedding_dim': 128,
    'latent_dim': 128,
    'epochs': 10,  # Reduced for demonstration; adjust as needed
    'batch_size': 32,
    'val_split': 0.2,
    'learning_rate': 1e-4,
    'mask_prob': 0.3,
    'error_threshold': 0.1,
    'early_stopping_patience': 5,
    'device': torch.device("cuda" if torch.cuda.is_available() else "cpu"),
}

# Load paired CSV data for a bitext set
def load_paired_csv(input_csv, english_csv):
    try:
        df_input = pd.read_csv(input_csv, header=None, names=['raw'])
        df_input['id'] = df_input['raw'].str[:8]
        df_input['text'] = df_input['raw'].str[8:].str.lstrip()
        
        df_english = pd.read_csv(english_csv, header=None, names=['raw'])
        df_english['id'] = df_english['raw'].str[:8]
        df_english['text'] = df_english['raw'].str[8:].str.lstrip()
        
        df_paired = pd.merge(df_input, df_english, on='id', suffixes=('_input', '_english'))
        return df_paired
    except Exception as e:
        logging.error(f"Error loading CSVs: {e}")
        raise

# Dataset with masking for autoencoder
class MaskedTextDataset(Dataset):
    def __init__(self, texts, tokenizer, max_length, mask_prob):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.texts = texts
        self.mask_prob = mask_prob
        self.mask_token_id = tokenizer.mask_token_id
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        encoding = self.tokenizer(
            text, padding="max_length", truncation=True, max_length=self.max_length, return_tensors="pt"
        )
        input_ids = encoding["input_ids"].squeeze(0)
        masked_input = input_ids.clone()
        mask = torch.rand(input_ids.shape) < self.mask_prob
        masked_input[mask] = self.mask_token_id
        return masked_input, input_ids

# Masked Autoencoder model
class MaskedAutoencoder(nn.Module):
    def __init__(self, vocab_size, max_length, embedding_dim, latent_dim):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4, dim_feedforward=latent_dim)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)
        decoder_layer = nn.TransformerDecoderLayer(d_model=embedding_dim, nhead=4, dim_feedforward=latent_dim)
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=2)
        self.output_layer = nn.Linear(embedding_dim, vocab_size)
    
    def forward(self, x):
        embedded = self.embedding(x).transpose(0, 1)
        encoded = self.encoder(embedded)
        decoded = self.decoder(encoded, encoded)
        logits = self.output_layer(decoded).transpose(0, 1)
        return logits

# Calculate reconstruction error
def calculate_reconstruction_error(model, tokenizer, text, max_length, device):
    encoding = tokenizer(
        text, padding="max_length", truncation=True, max_length=max_length, return_tensors="pt"
    )
    input_ids = encoding["input_ids"].to(device)
    model.eval()
    with torch.no_grad():
        logits = model(input_ids)
        probs = F.softmax(logits, dim=-1)
        predicted_ids = torch.argmax(probs, dim=-1)
    error = torch.mean((input_ids != predicted_ids).float()).item()
    return error

# Back-translation function
def back_translate(text, model_to_en, tokenizer_to_en, model_to_input, tokenizer_to_input, device):
    inputs = tokenizer_to_en(text, return_tensors="pt", padding=True, truncation=True, max_length=50).to(device)
    translated_ids = model_to_en.generate(**inputs)
    text_en = tokenizer_to_en.decode(translated_ids[0], skip_special_tokens=True)
    inputs_back = tokenizer_to_input(text_en, return_tensors="pt", padding=True, truncation=True, max_length=50).to(device)
    back_translated_ids = model_to_input.generate(**inputs_back)
    back_text = tokenizer_to_input.decode(back_translated_ids[0], skip_special_tokens=True)
    return back_text

# Language model scoring function
def score_sentence(text, lm_model, lm_tokenizer, device):
    inputs = lm_tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=50).to(device)
    with torch.no_grad():
        outputs = lm_model(**inputs, labels=inputs["input_ids"])
    return -outputs.loss.item()

# Train the autoencoder model
def train_model(model, train_loader, val_loader, criterion, optimizer, config):
    best_val_loss = float('inf')
    patience_counter = 0
    for epoch in range(config['epochs']):
        model.train()
        train_loss = 0
        for masked_input, target in tqdm(train_loader, desc=f"Epoch {epoch+1}/{config['epochs']}"):
            masked_input, target = masked_input.to(config['device']), target.to(config['device'])
            logits = model(masked_input)
            loss = criterion(logits.reshape(-1, config['vocab_size']), target.reshape(-1))
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        train_loss_avg = train_loss / len(train_loader)
        logging.info(f"Epoch {epoch+1} - Train Loss: {train_loss_avg:.6f}")
        val_loss = evaluate_model(model, val_loader, criterion, config)
        logging.info(f"Epoch {epoch+1} - Validation Loss: {val_loss:.6f}")
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= config['early_stopping_patience']:
                logging.info("Early stopping triggered.")
                break

# Evaluate the autoencoder model
def evaluate_model(model, val_loader, criterion, config):
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for masked_input, target in val_loader:
            masked_input, target = masked_input.to(config['device']), target.to(config['device'])
            logits = model(masked_input)
            loss = criterion(logits.reshape(-1, config['vocab_size']), target.reshape(-1))
            val_loss += loss.item()
    return val_loss / len(val_loader)

# Improve text using models
def improve_text(df_paired, model, tokenizer, config, translation_models, lm_models):
    improved_texts = []
    model_to_en, tokenizer_to_en = translation_models['to_en']
    model_to_input, tokenizer_to_input = translation_models['to_input']
    lm_model, lm_tokenizer = lm_models['lm']
    device = config['device']
    
    for idx, row in tqdm(df_paired.iterrows(), total=len(df_paired), desc="Improving texts"):
        sentence = row['text_input']
        error = calculate_reconstruction_error(model, tokenizer, sentence, config['max_length'], device)
        if error > config['error_threshold']:
            back_translated = back_translate(sentence, model_to_en, tokenizer_to_en, model_to_input, tokenizer_to_input, device)
            lm_original = score_sentence(sentence, lm_model, lm_tokenizer, device)
            lm_back = score_sentence(back_translated, lm_model, lm_tokenizer, device)
            improved_text = back_translated if lm_back > lm_original else sentence
        else:
            improved_text = sentence
        improved_texts.append({'id': row['id'], 'text_input': sentence, 'text_improved': improved_text, 'error': error})
    
    improved_df = pd.DataFrame(improved_texts)
    improved_df.to_csv(config['output_csv'], index=False)
    logging.info(f"Saved {len(improved_df)} improved texts to {config['output_csv']}.")

# Placeholder for training translation and language models
def train_translation_and_lm_models(df_paired, set_name):
    # Example models (replace with appropriate models for each language)
    model_to_en = MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-en-de").to(config['device'])
    tokenizer_to_en = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-de")
    model_to_input = MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-de-en").to(config['device'])
    tokenizer_to_input = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-de-en")
    
    lm_model = GPT2LMHeadModel.from_pretrained("gpt2").to(config['device'])
    lm_tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    
    # In practice, fine-tune these models with df_paired['text_input'] and df_paired['text_english']
    logging.info(f"Loaded placeholder translation and language models for {set_name}.")
    
    return {
        'to_en': (model_to_en, tokenizer_to_en),
        'to_input': (model_to_input, tokenizer_to_input),
        'lm': (lm_model, lm_tokenizer)
    }

# Main function to process all bitext sets
def main():
    for bitext_config in config['bitext_sets']:
        logging.info(f"Processing bitext set: {bitext_config['name']}")
        
        # Load data
        df_paired = load_paired_csv(bitext_config['input_csv'], bitext_config['english_csv'])
        logging.info(f"Loaded {len(df_paired)} paired rows for {bitext_config['name']}.")
        
        # Train or load translation and language models
        models_dict = train_translation_and_lm_models(df_paired, bitext_config['name'])
        translation_models = {'to_en': models_dict['to_en'], 'to_input': models_dict['to_input']}
        lm_models = {'lm': models_dict['lm']}
        
        # Initialize tokenizer and dataset for autoencoder
        tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
        config['vocab_size'] = tokenizer.vocab_size
        dataset = MaskedTextDataset(df_paired['text_input'].tolist(), tokenizer, config['max_length'], config['mask_prob'])
        train_size = int((1 - config['val_split']) * len(dataset))
        val_size = len(dataset) - train_size
        train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])
        train_loader = DataLoader(train_set, batch_size=config['batch_size'], shuffle=True)
        val_loader = DataLoader(val_set, batch_size=config['batch_size'])
        
        # Train autoencoder
        model = MaskedAutoencoder(config['vocab_size'], config['max_length'], config['embedding_dim'], config['latent_dim']).to(config['device'])
        criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)
        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])
        train_model(model, train_loader, val_loader, criterion, optimizer, config)
        torch.save(model.state_dict(), f"masked_autoencoder_{bitext_config['name']}.pth")
        logging.info(f"Autoencoder training complete for {bitext_config['name']}.")
        
        # Improve text and save results
        improve_text(df_paired, model, tokenizer, bitext_config, translation_models, lm_models)

if __name__ == "__main__":
    main()
